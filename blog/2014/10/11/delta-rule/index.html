
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Delta Rule再訪 - 読書帳</title>
  <meta name="author" content="Kenta OONO">

  
  <meta name="description" content="Back Propagationがわからなくなった Neural NetworkでのいわゆるDelta Ruleがいろいろな文脈で
最近のNeural Networkのアーキテクチャのバリエーションで学習に関しての記述で「これはBack Propagationでできる」と一言書いてあるのだけど、 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://delta2323.github.io/blog/2014/10/11/delta-rule">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="読書帳" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">


  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">読書帳</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:delta2323.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Delta Rule再訪</h1>
    
    
      <p class="meta">
	








  



	<time datetime="2014-10-11T21:43:45+09:00" pubdate data-updated="true">Oct 11<span>th</span>, 2014</time>
        
      </p>
    
  </header>


<div class="entry-content"><h2 id="back-propagation">Back Propagationがわからなくなった</h2>

<p>Neural NetworkでのいわゆるDelta Ruleがいろいろな文脈で
最近のNeural Networkのアーキテクチャのバリエーションで学習に関しての記述で「これはBack Propagationでできる」と一言書いてあるのだけど、なんでそれが正しいのかがわからないことがよくあったりする。</p>

<p>Back Propagationは損失関数をパラメータで偏微分した値が、DAGを逆向きに向かう方向に帰納的に求めることができるというもので、それを使ってSGD、AdaGrad、モーメント法などの方法でパラメータを更新することでNeural Networkを学習していくテクニックの事を言う。</p>

<p>上のようにhandyに書かれると「えっ」となる程度にはまだ扱い慣れていない。
本質的な計算は連鎖律だけなので、特別難しい事はしてないはず。
こういうのを理解するには一回自分で手を動かして見るに限る。</p>

<p>というわけで、Back Propagationがどういう時にできるのかを考えるために、適当な設定で実際にそれを導いてみる。</p>

<h2 id="section">問題設定</h2>

<p>まず問題設定を考える。ちょっと長いがここの部分が自分とこれを読んでもらう方々のコミュニケーションを行う基礎になるので丁寧に行うことにする。
（ちょっと冗長に書きすぎている気がするけれど、逆に問題設定をいい加減に書いて、計算式だけ書いている読者に伝わっていない論文や本の方が多いように思う。
記述量は少なくて一見簡単そうに見えるけれど、不正確に書かれているものと、記述量が多くて読むのに時間はかかるけれど、正確に書かれているものだったら、自分は後者の方がいいように思う。正確に書いていないのは読者のためではなく、正確に書けるほど理解できていないのではないかと穿ってみてしまう）。</p>

<p>$G = (V, E)$を有向グラフとし、GはDAGであるとする。
$v \in V$に対して$\mathrm{Pa}(v)\subset V$で$v$の親ノード、$\mathrm{Ch}(v)\subset V$で$v$の子ノードを表す。
親ノードがない（すなわち、$\mathrm{Pa}(v) = \emptyset$）ノードを入力ノード、子ノードがない（すなわち$\mathrm{Ch}(v) = \emptyset$）出力ノードとし、
入力ノード全体を$I \subset V$, 出力ノード全体を$O \subset V$で表す。</p>

<p>各エッジに対してそのエッジの重みを表す関数を$w: E \to \mathbb{R}$と書く。
$(u, v) \in E$に対して、$w(u, v)$を$w_{uv}$と書くことにする。</p>

<p>各ノードに対する入出力に関する文字設定を行っていく。
$x: V \to \mathbb{R}$をノードへの入力を表す関数、$y: V \to \mathbb{R}$をノードの出力を表す関数とする。
すなわち、ノード$v$には入力$x(v)$が入力され、$y(v)$が出力される。
この2つの関数を以下の方法で帰納的に決めていく。</p>

<p>入力ノードに入力$x_{I} \in \mathbb{R}^{|I|}$を与えて、DAG $G$の方向に沿って計算を進めていく（すなわちforward）していくことで出力を得る。
$x_{I}$を成分表示して、$x_{I} = (x_{v})_{v\in I}$とも書くことにする。</p>

<p>$v$が入力ノード、すなわち$v\in I$の時には $x(v) = x_{v}$とする。
$v$が入力ノードでない時には、$x(v) = \sum_{u\in \mathrm{Pa}(v)} w_{uv}h(u)$とする（$v$が入力ノードでない時には$\mathrm{Pa}(v)\not = \emptyset$であることに注意）。</p>

<p>入力関数$x$を用いて、出力関数$y$を$y(v) = \sigma(x(v))$とする。ここで、$\sigma : \mathbb{R} \to \mathbb{R}$は活性化関数である。</p>

<p>出力関数の一部$(y(v))_{v\in O}$をDAGの出力と思い、これを$y_{O}$と書く。
DAGとDAGの”外の世界”とのつながりを考えると、DAGには入力$x_{I}$を与えて、出力$y_{O}$を得ていると見ることができる。</p>

<p>以下では、$x(v), y(v)$をそれぞれ$x_{v}, y_{v}$と書く。
入力ノードに関しては$x_{v}$は、関数$x$からくる$x_{v}$と、DAG $G$に与える入力$x_{I}$の一部としての$x_{v}$の2通りがあるが上記の定義から、これは同一のものであるので誤解はない。出力ノードに関する$y_{v}$についても同様である。</p>

<p>損失関数を$L: \mathbb{R}^{|O|} \times \mathbb{R}^{|O|} \to \mathbb{R}$とする。
その出力結果と教師ラベル$t_{O} \in \mathbb{R}^{|O|}$と比較して損失$L(y_{O}, t_{O})$を得る。
以降$L$は$(y_{O}, t_{O})$で評価した値でしか使わないので、記号を簡単にするため、$L(y_{O}, t_{O})$を単に$L$と書いてしまう。</p>

<p>今計算したいものは、$G$が持つパラメータで、損失を微分した値である。
今回$G$が持つパラメータはノード間の重み$(w_{uv})_{(u, v) \in E}$なので、計算したいのは$\partial L / \partial w_{uv}$である
（以下では単なる勾配と言ったら、この値の事を指すことにする）。</p>

<h2 id="section-1">勾配・エラーを計算していく</h2>

<p>というわけで上記の設定を元に計算していきたいのが、その前に変数間の依存関係を確認しておく。Notationの定義をきちんとしていないが、3段だけ展開するとこうなる。</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
L
&= L(\{y_{v} | v\in O\}) \\ 
&= L(\{y_{v}(\{w_{uv}, y_{u}| u\in \mathrm{Pa}(v)\}) | v\in O\}) \\
&= L(\{y_{v}(\{w_{uv}, y_{u}(\{w_{wu}, y_{w}| w\in \mathrm{Pa}(u)\})| u\in \mathrm{Pa}(v)\}) | v\in O\}) \\ 
\end{align}
 %]]></script>

<p>天下り的だがここで誤差関数を定義する。各ノード$v$に対して、誤差$\delta_{v}\in \mathbb{R}$を
$\delta_{v} = \sigma ‘(v) \frac{\partial L}{\partial y_{v}}$で定義する。</p>

<p>Back Propagationというと、$\partial L / \partial w_{uv}$を帰納的に計算していくものと説明されることが多いが、
帰納的に計算されるのは誤差で、勾配は誤差から計算すると考えると理解しやすい。</p>

<p>まず、この式を愚直計算してみる。</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
\frac{\partial L}{\partial w_{uv}}
&= \frac{\partial y_{v}}{\partial w_{uv}} \frac{\partial L}{\partial y_{v}}\\
&= \frac{\partial h_{v}}{\partial w_{uv}} \frac{\partial y_{v}}{\partial h_{v}} \frac{\partial L}{\partial y_{v}}\\
&= y_{u} \sigma'(h_{v}) \frac{\partial L}{\partial y_{v}}\\
&= y_{u} \delta_{v}
\end{align}
 %]]></script>

<p>なので、勾配を計算するには、誤差を計算できればよい。</p>

<h3 id="base-step-v">Base Step: $v$が出力ノードの時</h3>

<p>今の問題設定では、活性化関数を微分した値$\sigma’$はforwardするときに計算できている。また、損失関数を出力ノードでの出力値で微分した値も計算できる。
従って、$v$が出力ノードの場合には、誤差$\delta_{v}$を計算でき、勾配も計算できる。</p>

<h3 id="inductive-step-v">Inductive Step: $v$が一般のノードの時</h3>

<p>$v$が一般のノード場合には、$\sigma’(h_{v})$は計算できるが、$\frac{\partial L}{\partial y_{v}}$の方は直接は計算出来ない。
そこで、この計算をもう少し進めることにする</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
\frac{\partial L}{\partial y_{v}}
&= \sum_{w\in \mathrm{Ch}(v)} \frac{\partial y_{w}}{\partial y_{v}} \frac{\partial L}{\partial y_{w}}\\
&= \sum_{w\in \mathrm{Ch}(v)} \frac{\partial h_{w}}{\partial y_{v}} \frac{\partial y_{w}}{\partial h_{w}} \frac{\partial L}{\partial y_{w}}\\
&= \sum_{w\in \mathrm{Ch}(v)} \frac{\partial h_{w}}{\partial y_{v}} \sigma'(h_{w}) \frac{\partial L}{\partial y_{w}}\\
&= \sum_{w\in \mathrm{Ch}(v)} w_{vw} \delta_{w}
\end{align}
 %]]></script>

<p>従って、次の式が得られる</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
\delta_{v} = \sigma'(h_{v}) \frac{\partial L}{\partial y_{v}}\\
&= \sigma'(h_{v}) \sum_{w\in \mathrm{Ch}(v)} w_{vw} \delta_{w}
\end{align}
 %]]></script>

<p>この式をよく見てみると、あるノード$v$の誤差$\delta_{v}$はその子ノード達の誤差によって計算することができることがわかる。
これが、「誤差をDAGとは逆向きに帰納的に計算していく」というBack Propagationである。</p>

<p>一度誤差が計算できてしまえば、あとは勾配は誤差に出力値をかければ得られる。</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Kenta OONO</span></span>

      








  


<time datetime="2014-10-11T21:43:45+09:00" pubdate data-updated="true">Oct 11<span>th</span>, 2014</time>
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://delta2323.github.io/blog/2014/10/11/delta-rule/" data-via="delta2323_" data-counturl="http://delta2323.github.io/blog/2014/10/11/delta-rule/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2014/10/09/experiment-oriented-program/" title="Previous Post: PFIセミナーで実験プログラム開発方法論について話しました">&laquo; PFIセミナーで実験プログラム開発方法論について話しました</a>
      
      
        <a class="basic-alignment right" href="/blog/2014/10/31/pydata-tokyo-meetup-1/" title="Next Post: PyData Tokyo MeetupでCaffeとmafについて話しました">PyData Tokyo MeetupでCaffeとmafについて話しました &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>大野健太（おおのけんた）<p/>
  <p>1986年生まれ、株式会社Preferred Networks所属、興味分野は機械学習・数学（純粋/応用両方）などです</p>
  <p>mailto: oono at preferredjp<br/>
  twitter: <a href="https://twitter.com/delta2323_">@delta2323_</a></p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/11/04/kraft-inequality/">Kraftの不等式</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/31/pydata-tokyo-meetup-1/">PyData Tokyo MeetupでCaffeとmafについて話しました</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/11/delta-rule/">Delta Rule再訪</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/09/experiment-oriented-program/">PFIセミナーで実験プログラム開発方法論について話しました</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/07/matplotlib-on-macos/">Matplotlib_on_macos</a>
      </li>
    
  </ul>
</section>




<section>
  <h1>Top Categories</h1>
    <ul id="top-category-list"><li><a href='/blog/categories/math'>math (12)</a></li><li><a href='/blog/categories/octopress'>octopress (8)</a></li><li><a href='/blog/categories/発表資料'>発表資料 (3)</a></li><li><a href='/blog/categories/general'>general (1)</a></li><li><a href='/blog/categories/emacs'>emacs (1)</a></li></ul>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Kenta OONO -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

</html>

